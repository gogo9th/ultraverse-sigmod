# Ultraverse Tutorial

## Purge Existing MySQL 
```console
$ sudo apt purge mysql-server mysql-common mariadb-server mariadb-common
$ sudo apt autoremove
$ sudo apt autoclean
$ sudo rm -rf /var/lib/mysql /var/lib/mysql.* /var/log/mysql /etc/mysql
```


## Install Required Software

**Build dependencies:**
```console
$ sudo apt install build-essential cmake pkg-config bison flex \
    libboost-all-dev libssl-dev zlib1g-dev libzstd-dev \
    libprotobuf-dev protobuf-compiler \
    libmysqlclient-dev libtbb-dev libtirpc-dev \
    golang-go
$ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28
```

**Optional - Esperanza benchmark scripts:**

Prerequisites:
- Python 3.10+
- Poetry ([installation guide](https://python-poetry.org/docs/#installing-with-the-official-installer))

```console
$ sudo apt install python3 libaio-dev

# Install Poetry (if not already installed)
$ curl -sSL https://install.python-poetry.org | python3 -

# Add Poetry to PATH (add to ~/.bashrc or ~/.zshrc)
$ echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc   # for bash
$ echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.zshrc    # for zsh
$ exec $SHELL   # reload shell to apply PATH changes

# Install Python dependencies via Poetry
$ cd scripts/esperanza
$ poetry install
$ cd ../..

# Ubuntu 24.04: create symlink for MySQL official binary compatibility
$ sudo ln -sf /usr/lib/x86_64-linux-gnu/libaio.so.1t64 /usr/lib/x86_64-linux-gnu/libaio.so.1
```

**Optional - Runtime optimization (jemalloc):**
```console
$ sudo apt install libjemalloc-dev
```



## Install and Setup MySQL

> **Note:** MariaDB is no longer supported. MySQL is required for both runtime and build (source tree needed for libbinlogevents).

```console
$ sudo apt install mysql-server mysql-client
```

Enable binary logging (check the actual including directory name specified in `/etc/mysql/my.cnf`).

```console
$ sudo vim /etc/mysql/mysql.conf.d/server.cnf
-------------------
   [mysqld]
   log-bin=myserver-binlog
   binlog_format=ROW
   binlog_row_image=FULL
   binlog_row_metadata=FULL
   binlog-checksum=NONE
   binlog_rows_query_log_events=ON
   max_binlog_size=300M
   plugin_load_add = ha_blackhole
   log_bin_trust_function_creators = 1
-------------------
```

Enable efficient large memory allocation.
```console
$ sudo vim /etc/systemd/system/multi-user.target.wants/mysql.service
---------------
   Environment="LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2"
---------------
```

Activate jemalloc & binary logging.
```console
$ sudo systemctl daemon-reload
$ sudo service mysql restart
```

Check that the binlog setup is alright.
```console
$ sudo mysql -u root -p
mysql> SHOW VARIABLES LIKE 'log_bin';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| log_bin       | ON    |
+---------------+-------+

mysql> SHOW VARIABLES LIKE 'log_bin_basename';
+------------------+-----------------------+
| Variable_name    | Value                 |
+------------------+-----------------------+
| log_bin_basename | /var/lib/mysql/binlog |
+------------------+-----------------------+

> SHOW VARIABLES LIKE 'version_malloc_library'; 
```

Add the default 'admin' user for Benchbase
```bash
sudo mysql
> CREATE USER 'admin'@'localhost' IDENTIFIED BY 'password';
> GRANT ALL PRIVILEGES ON *.* TO 'admin'@'localhost';
```


## Install Ultraverse

**Prerequisites:**
- CMake 3.28 or higher
- GCC 12+ or Clang 15+
- MySQL source tree (for libbinlogevents)

```console
$ git clone https://github.com/gogo9th/ultraverse-sigmod
$ cd ultraverse-sigmod
$ git submodule update --init --recursive

# Prepare MySQL source tree for libbinlogevents
$ git clone https://github.com/mysql/mysql-server.git --depth 1 --branch mysql-9.6.0
$ cmake -S ./mysql-server -B ./mysql-server/build   # generates autogenerated headers
$ cp ./mysql-server/build/include/my_config.h ./mysql-server/build/include/mysql_version.h ./mysql-server/include/

# Build Ultraverse
$ cmake -S . -B build -DCMAKE_BUILD_TYPE=Release
$ cmake --build build -j$(nproc)
```

**Optional - Use Clang instead of GCC:**
```console
$ CC=clang CXX=clang++ cmake -S . -B build
```

**Optional - Specify a custom MySQL source path:**
```console
$ cmake -S . -B build -DULTRAVERSE_MYSQLD_SRC_PATH=/path/to/mysql-server
```

## Quick Start (Standalone Tests)

Standalone scripts run workloads without BenchBase. They automatically download MySQL, generate data, execute transactions, and run Ultraverse state changes.

> **Note:** When running tests, `/etc/mysql/mysql.conf.d/server.cnf` should be empty because the test runs a fresh MySQL binary.

```console
$ cd scripts/esperanza
$ vim envfile
   export ULTRAVERSE_HOME=/path/to/ultraverse-sigmod/build/src
$ source envfile
$ rm -rf runs cache
$ poetry run python3 tpcc_standalone.py       # TPC-C workload
$ poetry run python3 epinions_standalone.py   # Epinions workload
```

For minishop (integration test with procedure patching):
```console
$ poetry run python3 minishop.py
$ poetry run python3 minishop_prepend.py   # tests rollback + prepend
```

**What the standalone scripts do:**
1. Download MySQL distribution (cached in `cache/`)
2. Initialize database and generate test data
3. Execute workload transactions
4. Generate state logs via `statelogd`
5. Run state change operations (rollback, replay, full-replay)
6. Validate results via table diff

**Configuration options** (edit the script's `__main__` block):
- `scale_factor` / `num_users`, `num_items`: Data size
- `query_count`: Number of transactions to execute

Results are stored in `runs/<workload>-<timestamp>/`.

### BenchBase Integration (Optional)

If you need to run with BenchBase for more comprehensive benchmarks:

```console
$ cd scripts/esperanza
$ vim envfile
   export ULTRAVERSE_HOME=/path/to/ultraverse-sigmod/build/src
   export BENCHBASE_HOME=/path/to/benchbase
$ source envfile
$ rm -rf runs cache
$ poetry run python3 epinions.py   # or: tpcc.py, tatp.py, seats.py, astore.py
```

See `scripts/esperanza/AGENTS.md` for detailed BenchBase setup instructions.

### Configuration File

Ultraverse uses a JSON configuration file for all settings. Create a config file (e.g., `ultraverse.json`):

```json
{
  "binlog": {
    "path": ".",
    "indexName": "myserver-binlog.index"
  },
  "stateLog": {
    "path": ".",
    "name": "benchbase"
  },
  "keyColumns": [
    "item2.i_id",
    "useracct.u_id",
    "review.i_id",
    "review.u_id",
    "trust.source_u_id",
    "trust.target_u_id"
  ],
  "columnAliases": {},
  "database": {
    "host": "127.0.0.1",
    "port": 3306,
    "name": "benchbase",
    "username": "admin",
    "password": "password"
  },
  "statelogd": {
    "threadCount": 0,
    "oneshotMode": true,
    "procedureLogPath": "",
    "developmentFlags": ["print-gids", "print-queries"]
  },
  "stateChange": {
    "threadCount": 0,
    "backupFile": "checkpoint-epinions.backup",
    "keepIntermediateDatabase": false,
    "rangeComparisonMethod": "eqonly"
  }
}
```

**Key Columns**: Ultraverse uses key columns for row-wise clustering and replay scheduling.
* Use **comma `,`** in the array to separate key groups (OR across groups).
* Use **plus `+`** inside a string to require all columns together (AND within a group).

Example:
```json
"keyColumns": ["orders.user_id+orders.item_id", "review.u_id"]
```

**Required Fields:**
* `stateLog.name` - State log file name (e.g., "benchbase")
* `database.name` - Target database name
* `keyColumns` - At least one key column for clustering

**Environment Variable Fallback** (used only when the JSON field is missing or empty):
* `DB_HOST`, `DB_PORT`, `DB_USER`, `DB_PASS` - Database connection
* `BINLOG_PATH` - Binary log base path

> **Note:** If database credentials are missing in both JSON and environment variables, execution will fail.


### Example: Retroactive Operation with Epinions Workload

The standalone script `epinions_standalone.py` automates the entire workflow: MySQL setup, workload execution, state log creation, and retroactive operations.

```console
$ cd scripts/esperanza
$ source envfile
$ poetry run python3 epinions_standalone.py
```

The script performs the following steps automatically:
1. Download and start MySQL (cached in `cache/`)
2. Create database, schema, and initial data
3. Execute Epinions workload transactions
4. Run `statelogd` to create state log from binary log
5. Run `make_cluster` to build clustering index
6. Perform retroactive rollback and replay operations
7. Verify results with table diff

Session files are saved in `runs/<session-name>/`.


### Complete Manual Example (Epinions)

This section provides a complete, copy-paste-ready example for running the entire Ultraverse workflow manually. Unlike the standalone scripts which automate everything, this guide lets you execute each step individually to understand the full process.

**Prerequisites:**
- MySQL is installed and running with binary logging enabled (see "Install and Setup MySQL" section above)
- Ultraverse is built (see "Install Ultraverse" section above)

**<u>Step 1.</u>** Create a working directory and set up environment.

```console
$ mkdir -p ~/ultraverse-tutorial && cd ~/ultraverse-tutorial
$ export ULTRAVERSE_HOME=/path/to/ultraverse-sigmod/build/src
```

**<u>Step 2.</u>** Create the database and schema.

```console
$ mysql -u admin -ppassword
```

```sql
-- Create database
CREATE DATABASE epinions;
USE epinions;

-- Create tables
CREATE TABLE useracct (
    u_id int NOT NULL PRIMARY KEY,
    name varchar(128) NOT NULL,
    email varchar(128) NOT NULL,
    creation_date datetime DEFAULT NULL
);

CREATE TABLE item2 (
    i_id int NOT NULL PRIMARY KEY,
    title varchar(128) NOT NULL,
    description varchar(512) DEFAULT NULL,
    creation_date datetime DEFAULT NULL
);

CREATE TABLE review (
    a_id int NOT NULL,
    u_id int NOT NULL,
    i_id int NOT NULL,
    rating int DEFAULT NULL,
    rank int DEFAULT NULL,
    comment varchar(256) DEFAULT NULL,
    creation_date datetime DEFAULT NULL,
    FOREIGN KEY (u_id) REFERENCES useracct (u_id) ON DELETE CASCADE,
    FOREIGN KEY (i_id) REFERENCES item2 (i_id) ON DELETE CASCADE
);
CREATE INDEX idx_rating_uid ON review (u_id);
CREATE INDEX idx_rating_iid ON review (i_id);

CREATE TABLE trust (
    source_u_id int NOT NULL,
    target_u_id int NOT NULL,
    trust int NOT NULL,
    creation_date datetime DEFAULT NULL,
    FOREIGN KEY (source_u_id) REFERENCES useracct (u_id) ON DELETE CASCADE,
    FOREIGN KEY (target_u_id) REFERENCES useracct (u_id) ON DELETE CASCADE
);
CREATE INDEX idx_trust_sid ON trust (source_u_id);
CREATE INDEX idx_trust_tid ON trust (target_u_id);
```

**<u>Step 3.</u>** Insert initial data (this will be our baseline state).

```sql
-- Create users
INSERT INTO useracct VALUES (1, 'Alice', 'alice@example.com', NOW());
INSERT INTO useracct VALUES (2, 'Bob', 'bob@example.com', NOW());
INSERT INTO useracct VALUES (3, 'Charlie', 'charlie@example.com', NOW());

-- Create items
INSERT INTO item2 VALUES (1, 'Laptop', 'High-performance laptop', NOW());
INSERT INTO item2 VALUES (2, 'Phone', 'Smartphone with great camera', NOW());
```

**<u>Step 4.</u>** Create a checkpoint (backup) and reset binary logs.

```console
# Create baseline backup
$ mysqldump -u admin -ppassword epinions > dbdump.sql

# Flush binary logs (so GID starts from 0)
$ mysql -u admin -ppassword -e "PURGE BINARY LOGS BEFORE NOW();"
```

**<u>Step 5.</u>** Execute workload transactions (these generate binary log entries).

```console
$ mysql -u admin -ppassword epinions
```

```sql
-- GID 0: Add review by Alice
INSERT INTO review VALUES (1, 1, 1, 5, 1, 'Excellent product!', NOW());

-- GID 1: Add review by Bob
INSERT INTO review VALUES (2, 2, 1, 4, 2, 'Good but expensive', NOW());

-- GID 2: Add review by Alice for Phone
INSERT INTO review VALUES (3, 1, 2, 3, 1, 'Average phone', NOW());

-- GID 3: Add trust relationship
INSERT INTO trust VALUES (1, 2, 8, NOW());

-- GID 4: Add another trust relationship
INSERT INTO trust VALUES (2, 3, 7, NOW());

-- GID 5: Update user name
UPDATE useracct SET name = 'Alice Smith' WHERE u_id = 1;

-- GID 6: Update review rating
UPDATE review SET rating = 4 WHERE u_id = 1 AND i_id = 1;

-- GID 7: Update trust value
UPDATE trust SET trust = 9 WHERE source_u_id = 1 AND target_u_id = 2;
```

**<u>Step 6.</u>** Copy binary log files to working directory.

```console
# Check binary log location
$ mysql -u admin -ppassword -e "SHOW VARIABLES LIKE 'log_bin_basename';"

# Copy binary logs (adjust path based on your MySQL configuration)
$ sudo cp /var/lib/mysql/myserver-binlog.* ~/ultraverse-tutorial/
$ sudo chown $(whoami):$(whoami) ~/ultraverse-tutorial/myserver-binlog.*
```

**<u>Step 7.</u>** Create the Ultraverse configuration file.

```console
$ cat > ultraverse.json << 'EOF'
{
  "binlog": {
    "path": ".",
    "indexName": "myserver-binlog.index"
  },
  "stateLog": {
    "path": ".",
    "name": "epinions"
  },
  "keyColumns": [
    "item2.i_id",
    "useracct.u_id",
    "review.u_id+review.i_id",
    "trust.source_u_id+trust.target_u_id"
  ],
  "columnAliases": {},
  "database": {
    "host": "127.0.0.1",
    "port": 3306,
    "name": "epinions",
    "username": "admin",
    "password": "password"
  },
  "statelogd": {
    "threadCount": 0,
    "oneshotMode": true,
    "procedureLogPath": "",
    "developmentFlags": ["print-gids", "print-queries"]
  },
  "stateChange": {
    "threadCount": 0,
    "backupFile": "dbdump.sql",
    "keepIntermediateDatabase": false,
    "rangeComparisonMethod": "eqonly"
  }
}
EOF
```

**<u>Step 8.</u>** Generate state log from binary log.

```console
$ $ULTRAVERSE_HOME/statelogd -c ultraverse.json
```

Output files: `epinions.ultstatelog`, `epinions.ultindex`

**<u>Step 9.</u>** Create cluster map and table map.

```console
$ $ULTRAVERSE_HOME/db_state_change ultraverse.json make_cluster
```

Output files: `epinions.ultcluster`, `epinions.ultcolumns`, `epinions.ulttables`

**<u>Step 10.</u>** View the state log to understand the transaction structure.

```console
$ $ULTRAVERSE_HOME/state_log_viewer -i epinions -v
```

**<u>Step 11.</u>** Perform a retroactive rollback.

In this example, we'll rollback GID 1 (Bob's review). This demonstrates how Ultraverse selectively replays dependent transactions.

```console
# Prepare rollback plan
$ $ULTRAVERSE_HOME/db_state_change ultraverse.json rollback=1

# Execute replay (--replay-from 0 replays all transactions from the backup)
$ $ULTRAVERSE_HOME/db_state_change --replay-from 0 ultraverse.json replay
```

**<u>Step 12.</u>** Verify the result.

```console
$ mysql -u admin -ppassword -e "SELECT * FROM epinions.review;"
```

Bob's review (a_id=2) should be removed, and other reviews should remain intact.


### Manual Workflow (Step-by-Step)

If you need to run each step manually, follow the instructions below.

**<u>Step 1.</u>** Read binary log and create state log.

```console
$ ../src/statelogd -c ultraverse.json
```

Options:
- `-c file` - JSON config file (required)
- `-v` - DEBUG log level
- `-V` - TRACE log level

Output files: `benchbase.ultstatelog`, `benchbase.ultchkpoint`, `benchbase.ultindex`

> **Note:** Checkpoint serialization is currently disabled. The `.ultchkpoint` file is created but empty.

**<u>Step 2.</u>** Make a cluster map & table map before performing a state change.

```console
$ ../src/db_state_change ultraverse.json make_cluster
```
The output files are as follows: `benchbase.ulttables`, `benchbase.ultindex`, `benchbase.ultcolumns`, and `benchbase.ultcluster`.

**<u>Step 3.</u>** Prepare the state change (rollback/prepend). This generates `benchbase.ultreplayplan` in the state log path.

**Rollback only:**
```console
$ ../src/db_state_change ultraverse.json rollback=2,32
```

**Prepend user query before a specific GID:**
```console
$ echo "UPDATE useracct SET name = 'HELOWRLD' WHERE u_id = 512;" > prepend.sql
$ ../src/db_state_change ultraverse.json prepend=2,prepend.sql
```

**Combined (prepend + rollback):**
```console
$ ../src/db_state_change ultraverse.json prepend=2,prepend.sql:rollback=32
```

You can also specify GID range or skip specific GIDs:
```console
$ ../src/db_state_change --gid-range 1...100 --skip-gids 5,10,15 ultraverse.json rollback=2
```

Output file: `benchbase.ultreplayplan`

**<u>Step 4.</u>** Replay using the generated `.ultreplayplan`.

```console
$ ../src/db_state_change ultraverse.json replay
```

**Alternative: Full sequential replay** (replays all transactions except rollback GIDs):
```console
$ ../src/db_state_change ultraverse.json full-replay
```

Options:
- Set `keepIntermediateDatabase: true` in config to preserve the intermediate database after replay.
- Use `--no-exec-replace-query` to print replace queries without executing them.


### Viewing State Logs

Use `state_log_viewer` to inspect the contents of a state log file:

```console
$ ../src/state_log_viewer -i benchbase
```

Options:
- `-i statelog` - State log file name (required)
- `-s startgid` - Start GID for filtering
- `-e endgid` - End GID for filtering
- `-v` - Print additional info (itemset, whereset)
- `-V` - Print more info (beforehash, afterhash)


### CLI Options Reference

**db_state_change options:**

| Option | Description |
|--------|-------------|
| `--gid-range START...END` | Process only GIDs in range |
| `--skip-gids GID1,GID2,...` | Skip specific GIDs |
| `--replay-from GID` | Pre-replay from GID before main replay |
| `--no-exec-replace-query` | Print replace queries without executing |
| `--dry-run` | Dry run mode |
| `-v` | DEBUG log level |
| `-V` | TRACE log level |

**Environment variables:**

| Variable | Description |
|----------|-------------|
| `ULTRAVERSE_REPORT_NAME` | Custom report file name (optional) |

**Actions (can be chained with `:`):**

| Action | Description |
|--------|-------------|
| `make_cluster` | Create cluster/index files |
| `rollback=gid1,gid2,...` | Mark GIDs for rollback |
| `rollback=-` | Read GIDs from stdin |
| `prepend=gid,sqlfile` | Prepend SQL before GID |
| `replay` | Execute replay plan |
| `full-replay` | Sequential replay (skip rollback GIDs) |
| `auto-rollback=ratio` | Auto-select rollback targets by ratio (fraction of in-scope GIDs) |

Notes:
- `auto-rollback` runs analysis only (no intermediate DB); it reports selected `rollbackGids` for later `rollback=...`.
- In-scope GIDs are filtered by database match, `--gid-range`, and `--skip-gids`.


### Understanding `--replay-from`

The `--replay-from` option handles a gap between the backup checkpoint and the rollback target. **Pre-replay only runs when `--replay-from` is explicitly provided** (default is unset, meaning no pre-replay).

**Scenario:** You have a backup at GID 36, and you want to rollback GID 71.

```
Backup (GID 36)   Target (GID 71)     Latest
    |                  |                 |
    v                  v                 v
----+------------------+-----------------+---->
    |<-- Pre-replay -->|<-- Selective -->|
        (GID 37~70)         Replay
```

- **GIDs 37~70 (Before Target):** These are not rollback targets, but they must be executed sequentially to restore the database state to GID 70—the prerequisite for rolling back GID 71.
- **GIDs 72~End (After Target):** This is where Ultraverse's optimization applies. Only dependent queries are selectively replayed.

**Usage:**
```console
$ ../src/db_state_change ultraverse.json rollback=71
$ ../src/db_state_change --replay-from 37 ultraverse.json replay
```

If `--replay-from 0` is specified, all GIDs from 0 to the target are pre-replayed before the main replay phase.

> **Note:** The standalone test scripts use `--replay-from 0` by default to ensure complete replay from the backup checkpoint.


### Tuning: Replay GC Interval

During replay, `RowGraph` runs a garbage collection (GC) thread that periodically removes completed nodes from memory. The GC interval is hardcoded in `src/mariadb/state/new/StateChanger.replay.cpp`.

**Current default:** 10 seconds (10000ms)

**Locations to modify:**
- **Pre-replay GC** (line ~137): `std::this_thread::sleep_for(std::chrono::milliseconds(10000));`
- **Main replay GC** (line ~276): `std::this_thread::sleep_for(std::chrono::milliseconds(10000));`

**Tuning guidelines:**
| Interval | Use Case |
|----------|----------|
| Shorter (1-5s) | High memory pressure, many small transactions |
| Default (10s) | Balanced for most workloads |
| Longer (15-30s) | Low memory pressure, reduced GC overhead |

> **Note:** GC is stop-the-world—it pauses worker threads briefly. Shorter intervals may reduce peak memory but increase pause frequency.

After modifying, rebuild:
```console
$ cmake --build build --target db_state_change
```


## MySQL Useful Commands


#### Create a User

```console
CREATE USER 'admin'@'localhost' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON *.* TO 'admin'@'localhost';
```

#### Run an SQL script
```console
$ mysql -u root
> source /home/skyer/Desktop/script.sql;
```


#### Print SQL variables
```console
> SELECT VARIABLES LIKE "<var name>";
```

#### Print the sizes of all databases
```console
> SELECT table_schema, ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) AS "Size (MB)"
> FROM information_schema.TABLES
> GROUP BY table_schema;
```



#### Print the sizes of all tables
```console
> SELECT table_schema, table_name, round(((data_length + index_length) / 1024 / 1024), 2) `Size in MB`
> FROM information_schema.TABLES
> ORDER BY (data_length + index_length) DESC;
```



#### Enable (or reset) the general log
```
$ sudo rm -f /var/log/mysql/mylog
$ mysql -uroot -p123456 -e "set global general_log=0; set global general_log=1; set global general_log_file='/var/log/mysql/mylog';"
```

#### Configure DBMS server's listening IP address
```console
$ vim /etc/mysql/my.cnf
> comment out 'bind-address = 182.162.21.181' or set it to the listening (allowed) IP address
$ sudo service mysql restart # or 'mysql' in case of MySQL

 # check that the deoman is listening to all IPs (or only the bound IP)
$ sudo netstat -alpn | grep mysqld # or 'mysqld' in case of MySQL 
```




#### Change the DBMS's database directory
- Link: [https://www.digitalocean.com/community/tutorials/how-to-move-a-mysql-data-directory-to-a-new-location-on-ubuntu-16-04](https://www.digitalocean.com/community/tutorials/how-to-move-a-mysql-data-directory-to-a-new-location-on-ubuntu-16-04)

```console
$ sudo service mysql stop
$ sudo rsync -av /var/lib/mysql /mnt/volume-nyc1-01 # move the DB
$ sudo mv /var/lib/mysql /var/lib/mysql.bak # invalidate the DB
$ sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf
------------
   +++ datadir=/mnt/volume-nyc1-01/mysql
   +++[mysqld]
   +++disable_log_bin
------------
$ sudo vim /etc/apparmor.d/tunables/alias
------------
   +++ [label /etc/apparmor.d/tunables/alias]
   +++ alias /var/lib/mysql/ -> /mnt/volume-nyc1-01/mysql/,
------------
$ sudo systemctl restart apparmor
$ sudo mkdir /var/lib/mysql/mysql -p
$ sudo systemctl start mysql
$ mysql -u root -p
```



#### Purge broken MariaDB (10.3)
- Link: [https://askubuntu.com/questions/946646/install-of-mysql-server-after-mariadb-fails/948428#948428](https://askubuntu.com/questions/946646/install-of-mysql-server-after-mariadb-fails/948428#948428)
```console
$ apt search mariadb | grep "\[install"
$ apt search mysql | grep "\[install"
$ sudo dpkg --force depends --purge <package> <package> ...
$ sudo rm -rf /var/lib/mysql* /etc/mysql
$ sudo apt-get --fix-broken install
$ sudo apt autoremove
$ sudo reboot
$ sudo apt-get install mariadb-server
```

#### Root login failure (or lost root login)
- Error message: ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)

- Solution: [https://stackoverflow.com/questions/17975120/access-denied-for-user-rootlocalhost-using-password-yes-no-privileges](https://stackoverflow.com/questions/17975120/access-denied-for-user-rootlocalhost-using-password-yes-no-privileges)

```console
$ sudo service mysql stop # or ps aux | grep mysql & kill it
$ sudo mysqld --skip-grant-tables
$ mysql -u root
> use mysql;
> update user set password=PASSWORD("root") where User='root';
> flush privileges;
$ sudo killall mysqld
$ sudo service mysql start
$ mysql -u root -p
```
